{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae5bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a83438f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-twitter\\glove.twitter.27B.100d.txt\n",
      "glove-twitter\\glove.twitter.27B.200d.txt\n",
      "glove-twitter\\glove.twitter.27B.25d.txt\n",
      "glove-twitter\\glove.twitter.27B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk(\"glove-twitter\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "012f51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#twitter = pd.read_csv(\"preprocessed_data.csv\")    #normal one used before\n",
    "#twitter = pd.read_csv(\"sorted_preprocessed_data.csv\")  #with lowest emotion scores\n",
    "twitter = pd.read_csv(\"final_dataset.csv\")\n",
    "#twitter = pd.read_csv(\"greater_than_some_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0e0e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>people who post add me on snapchat must be deh...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>peopl who post add me on snapchat must be dehy...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>brianklaas as we see trump is dangerous to fre...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>brianklaa as we see trump is danger to freepre...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>now issa is stalking tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>now issa is stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>riskshow thekevinallison thx for the best time...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>riskshow thekevinallison thx for the best time...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>still waiting on those supplies liscus</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>still wait on those suppli liscu</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451386</th>\n",
       "      <td>94</td>\n",
       "      <td>0x321566</td>\n",
       "      <td>im so happy nowonder the name of this show hap...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>im so happi nowond the name of thi show happi ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451387</th>\n",
       "      <td>627</td>\n",
       "      <td>0x38959e</td>\n",
       "      <td>in every circumtance id like to be thankful to...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>in everi circumt id like to be thank to the al...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451388</th>\n",
       "      <td>274</td>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>theres currently two girls walking around the ...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>there current two girl walk around the librari...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451389</th>\n",
       "      <td>840</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>ah corporate life where you can date  using ju...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>ah corpor life where you can date use just the...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451390</th>\n",
       "      <td>360</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>blessed to be living sundayvibes</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>bless to be live sundayvib</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1451391 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score  tweet_id                                               text  \\\n",
       "0           391  0x376b20  people who post add me on snapchat must be deh...   \n",
       "1           433  0x2d5350  brianklaas as we see trump is dangerous to fre...   \n",
       "2           376  0x1cd5b0                    now issa is stalking tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚    \n",
       "3           120  0x1d755c  riskshow thekevinallison thx for the best time...   \n",
       "4          1021  0x2c91a8            still waiting on those supplies liscus    \n",
       "...         ...       ...                                                ...   \n",
       "1451386      94  0x321566  im so happy nowonder the name of this show hap...   \n",
       "1451387     627  0x38959e  in every circumtance id like to be thankful to...   \n",
       "1451388     274  0x2cbca6  theres currently two girls walking around the ...   \n",
       "1451389     840  0x24faed  ah corporate life where you can date  using ju...   \n",
       "1451390     360  0x34be8c                  blessed to be living sundayvibes    \n",
       "\n",
       "        identification       emotion  \\\n",
       "0                train  anticipation   \n",
       "1                train       sadness   \n",
       "2                train          fear   \n",
       "3                train           joy   \n",
       "4                train  anticipation   \n",
       "...                ...           ...   \n",
       "1451386          train           joy   \n",
       "1451387          train           joy   \n",
       "1451388          train           joy   \n",
       "1451389          train           joy   \n",
       "1451390          train           joy   \n",
       "\n",
       "                                              text_stemmed  Category  \n",
       "0        peopl who post add me on snapchat must be dehy...         4  \n",
       "1        brianklaa as we see trump is danger to freepre...         8  \n",
       "2                              now issa is stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚         1  \n",
       "3        riskshow thekevinallison thx for the best time...         7  \n",
       "4                         still wait on those suppli liscu         4  \n",
       "...                                                    ...       ...  \n",
       "1451386  im so happi nowond the name of thi show happi ...         7  \n",
       "1451387  in everi circumt id like to be thank to the al...         7  \n",
       "1451388  there current two girl walk around the librari...         7  \n",
       "1451389  ah corpor life where you can date use just the...         7  \n",
       "1451390                         bless to be live sundayvib         7  \n",
       "\n",
       "[1451391 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592b6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = twitter[\"text_stemmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be717266",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = twitter[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200d3ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          peopl who post add me on snapchat must be dehy...\n",
       "1          brianklaa as we see trump is danger to freepre...\n",
       "2                                now issa is stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚\n",
       "3          riskshow thekevinallison thx for the best time...\n",
       "4                           still wait on those suppli liscu\n",
       "                                 ...                        \n",
       "1451386    im so happi nowond the name of thi show happi ...\n",
       "1451387    in everi circumt id like to be thank to the al...\n",
       "1451388    there current two girl walk around the librari...\n",
       "1451389    ah corpor life where you can date use just the...\n",
       "1451390                           bless to be live sundayvib\n",
       "Name: text_stemmed, Length: 1451391, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a432f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZER\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d699c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13426  1217   144    15]\n",
      " [   32     4 72140   584]\n",
      " [    8  3622  2152   924]\n",
      " ...\n",
      " [   54   799  3334 29349]\n",
      " [ 1769    15   675    12]\n",
      " [    2    16   125 10698]]\n"
     ]
    }
   ],
   "source": [
    "#Creating Vectors Using Padding\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eac2e64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451391"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd78cdf",
   "metadata": {},
   "source": [
    "## Glove Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e201ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open(\"glove-twitter/glove.twitter.27B.200d.txt\",encoding=\"utf8\")\n",
    "for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3f822f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue here: https://www.kaggle.com/hassanamin/glove-based-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcb8045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 200))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbea1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 200)            171581000 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 801       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,581,801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 171,581,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d4b6b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4.385655\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b73cfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# epochs = 50\n",
    "# batch_size = 512 #512\n",
    "\n",
    "# history = model.fit(padded_docs, labels, epochs=epochs, batch_size=batch_size)#, \n",
    "#                     #validation_split=0.1, callbacks=[tf.keras.callbacks.EarlyStopping\n",
    "#                     #(monitor='val_loss', patience=1, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ee94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685acce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad57e1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d00ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c6f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8b6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eb79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f59eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be27ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4067f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9ce99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a89179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
