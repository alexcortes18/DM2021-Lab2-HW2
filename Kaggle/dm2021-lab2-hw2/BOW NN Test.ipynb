{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e942277d",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be680bdd",
   "metadata": {},
   "source": [
    "#### Upload the CSV created from Kaggle Competition Preprocessing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012f51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#twitter = pd.read_csv(\"preprocessed_data.csv\")    #normal one used before\n",
    "#twitter = pd.read_csv(\"sorted_preprocessed_data.csv\")  #with lowest emotion scores\n",
    "#twitter = pd.read_csv(\"greater_than_some_score.csv\")\n",
    "twitter = pd.read_csv(\"final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0e0e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>883</td>\n",
       "      <td>0x292d69</td>\n",
       "      <td>forever daddys little girl  when daddy goes lo...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>forev daddi littl girl when daddi goe look for...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>391</td>\n",
       "      <td>0x271dff</td>\n",
       "      <td>the day before a big event and my team has eve...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>the day befor a big event and my team ha every...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507</td>\n",
       "      <td>0x305ac3</td>\n",
       "      <td>nikkinicolex i was going to do that this year ðŸ˜‚ðŸ˜©</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>nikkinicolex i wa go to do that thi year ðŸ˜‚ðŸ˜©</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>560</td>\n",
       "      <td>0x2415c8</td>\n",
       "      <td>donaldtrump    of obstruction of justice and i...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>donaldtrump of obstruct of justic and interfer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>348</td>\n",
       "      <td>0x378a4c</td>\n",
       "      <td>is holy and just therefore he must hate and p...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>is holi and just therefor he must hate and pun...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score  tweet_id                                               text  \\\n",
       "0     883  0x292d69  forever daddys little girl  when daddy goes lo...   \n",
       "1     391  0x271dff  the day before a big event and my team has eve...   \n",
       "2     507  0x305ac3  nikkinicolex i was going to do that this year ðŸ˜‚ðŸ˜©    \n",
       "3     560  0x2415c8  donaldtrump    of obstruction of justice and i...   \n",
       "4     348  0x378a4c   is holy and just therefore he must hate and p...   \n",
       "\n",
       "  identification       emotion  \\\n",
       "0          train       sadness   \n",
       "1          train         trust   \n",
       "2          train       sadness   \n",
       "3          train       sadness   \n",
       "4          train  anticipation   \n",
       "\n",
       "                                        text_stemmed  Category  \n",
       "0  forev daddi littl girl when daddi goe look for...         8  \n",
       "1  the day befor a big event and my team ha every...         2  \n",
       "2        nikkinicolex i wa go to do that thi year ðŸ˜‚ðŸ˜©         8  \n",
       "3  donaldtrump of obstruct of justic and interfer...         8  \n",
       "4  is holi and just therefor he must hate and pun...         4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_rows', 1000)\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d5f30",
   "metadata": {},
   "source": [
    "#### Dividing the dataframe into a train and a test sections. For the input section I tried with both \"text\" and \"text_stemmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20743eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(twitter.text,twitter.emotion,\n",
    "                                                test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a2ca8",
   "metadata": {},
   "source": [
    "#### Make a BOW for the top 20k - 25k (best results were in this range) max features. Using nltk.word_tokenize to accept emojis in the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f548482d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=25000,\n",
       "                tokenizer=<function word_tokenize at 0x000001AA8618FEE0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW = CountVectorizer(tokenizer=nltk.word_tokenize, max_features=25000)\n",
    "\n",
    "BOW.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10b3ae",
   "metadata": {},
   "source": [
    "#### Transform the \"text\" for both train and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b113931",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = BOW.transform(x_train)\n",
    "#y_train = y_train\n",
    "\n",
    "x_test = BOW.transform(x_test)\n",
    "#y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d7bdc",
   "metadata": {},
   "source": [
    "#### Use 1 hot encoding to deal with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b365c7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d476f2bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  25000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = x_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ddf39",
   "metadata": {},
   "source": [
    "#### Now I proceed to build the model. The lab model with hyperparameters modifications gave the best result. Even so, I also tried several other models you can see in the following pictures, but unfortunately this one gave me the best result. I also added another hidden layer, too.\n",
    "\n",
    "Other tries:\n",
    "![Snapshot](Models_tried.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad041029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 25000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                800032    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 802,408\n",
      "Trainable params: 802,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=32)(X)  # Original: 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=32)(H1)  # Original: 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3rd hidden layer   (CREATED BY ME)\n",
    "H1_W4 = Dense(units=32)(H2)  # Original: 64\n",
    "H4 = ReLU()(H1_W4)\n",
    "\n",
    "# output layer\n",
    "#Original output layer\n",
    "# H2_W3 = Dense(units=output_shape)(H2)  # 8\n",
    "# H3 = Softmax()(H2_W3)\n",
    "\n",
    "H2_W3 = Dense(units=output_shape)(H4)  # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba4c3b",
   "metadata": {},
   "source": [
    "#### I tried also some different hyperparameters here, and added Early Stopping, which allow us 2 things. One is to stop overfitting by stopping the unnecesary epochs (when validation loss starts to increase), and also decreases training time by reducing those unnecesary epochs. In this model only 2 epochs run before it overfits, so then the model stops training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182a9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexc\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10230/18143 [===============>..............] - ETA: 1:38 - loss: 1.3329 - accuracy: 0.5195"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "\n",
    "epochs = 5 #25\n",
    "batch_size = 64 #32, 100\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max')\n",
    "callbacks = [es] #Early Stopping\n",
    "\n",
    "# training!\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = (x_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc883c",
   "metadata": {},
   "source": [
    "#### Predict on 'testing' data to see its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = model.predict(x_test, batch_size=128) #128\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7974a4",
   "metadata": {},
   "source": [
    "#### Decode it to see the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779af860",
   "metadata": {},
   "source": [
    "#### Final training set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 4)))\n",
    "\n",
    "#Best until now 0.5526"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfe230",
   "metadata": {},
   "source": [
    "## Predicting on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d513d",
   "metadata": {},
   "source": [
    "#### Now i just upload the twitter_test_data.csv we got from the Kaggle Competition Preprocessing jupyter notebook and apply the recently acquired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdfb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_test_data = pd.read_csv(\"twitter_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cd77f",
   "metadata": {},
   "source": [
    "#### Transforming the testing data into a BOW and then predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_twitter = BOW.transform(twitter_test_data['text'])\n",
    "\n",
    "pred_result_test_data = model.predict(x_test_twitter, batch_size=128)\n",
    "\n",
    "print('x_test.shape: ', x_test_twitter.shape)\n",
    "pred_result_test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add55598",
   "metadata": {},
   "source": [
    "#### Decoding the values to then be able to make a dataframe with meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result_test_data = label_decode(label_encoder, pred_result_test_data)\n",
    "pred_result_test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da918171",
   "metadata": {},
   "source": [
    "#### Making the DataFrame that will be uploaded to KAGGLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ea1f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "upload_df = pd.DataFrame(columns = [[\"id\",\"emotion\"]])\n",
    "upload_df[\"id\"] = twitter_test_data[\"tweet_id\"]\n",
    "upload_df[\"emotion\"] = pred_result_test_data\n",
    "upload_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_df['emotion']\n",
    "upload_df.emotion.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88680b2c",
   "metadata": {},
   "source": [
    "## Final Upload to KAGGLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6be332",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_df.to_csv(\"./uploads/Keras_25k.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554eeb2",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f8c60",
   "metadata": {},
   "source": [
    "#### Extra try with logistic regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fb979",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0243a8",
   "metadata": {},
   "source": [
    "Use this one with only the BOW and the x_train, y_train BOW transformed. (But in the target here i needed the category column)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6fa85",
   "metadata": {},
   "source": [
    "##### To use this one, change \"twitter.emotion\" to \"twitter.Category\" in the train-split data part, and avoid modeling the data with the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48894fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
