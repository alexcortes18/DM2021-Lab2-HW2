{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012f51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#twitter = pd.read_csv(\"preprocessed_data.csv\")    #normal one used before\n",
    "#twitter = pd.read_csv(\"sorted_preprocessed_data.csv\")  #with lowest emotion scores\n",
    "twitter = pd.read_csv(\"final_dataset.csv\")\n",
    "#twitter = pd.read_csv(\"greater_than_some_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0e0e90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>people who post add me on snapchat must be deh...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>peopl who post add me on snapchat must be dehy...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>brianklaas as we see trump is dangerous to fre...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>brianklaa as we see trump is danger to freepre...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>now issa is stalking tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>now issa is stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>riskshow thekevinallison thx for the best time...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>riskshow thekevinallison thx for the best time...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>still waiting on those supplies liscus</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>still wait on those suppli liscu</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451386</th>\n",
       "      <td>94</td>\n",
       "      <td>0x321566</td>\n",
       "      <td>im so happy nowonder the name of this show hap...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>im so happi nowond the name of thi show happi ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451387</th>\n",
       "      <td>627</td>\n",
       "      <td>0x38959e</td>\n",
       "      <td>in every circumtance id like to be thankful to...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>in everi circumt id like to be thank to the al...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451388</th>\n",
       "      <td>274</td>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>theres currently two girls walking around the ...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>there current two girl walk around the librari...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451389</th>\n",
       "      <td>840</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>ah corporate life where you can date  using ju...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>ah corpor life where you can date use just the...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451390</th>\n",
       "      <td>360</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>blessed to be living sundayvibes</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>bless to be live sundayvib</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1451391 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score  tweet_id                                               text  \\\n",
       "0           391  0x376b20  people who post add me on snapchat must be deh...   \n",
       "1           433  0x2d5350  brianklaas as we see trump is dangerous to fre...   \n",
       "2           376  0x1cd5b0                    now issa is stalking tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚    \n",
       "3           120  0x1d755c  riskshow thekevinallison thx for the best time...   \n",
       "4          1021  0x2c91a8            still waiting on those supplies liscus    \n",
       "...         ...       ...                                                ...   \n",
       "1451386      94  0x321566  im so happy nowonder the name of this show hap...   \n",
       "1451387     627  0x38959e  in every circumtance id like to be thankful to...   \n",
       "1451388     274  0x2cbca6  theres currently two girls walking around the ...   \n",
       "1451389     840  0x24faed  ah corporate life where you can date  using ju...   \n",
       "1451390     360  0x34be8c                  blessed to be living sundayvibes    \n",
       "\n",
       "        identification       emotion  \\\n",
       "0                train  anticipation   \n",
       "1                train       sadness   \n",
       "2                train          fear   \n",
       "3                train           joy   \n",
       "4                train  anticipation   \n",
       "...                ...           ...   \n",
       "1451386          train           joy   \n",
       "1451387          train           joy   \n",
       "1451388          train           joy   \n",
       "1451389          train           joy   \n",
       "1451390          train           joy   \n",
       "\n",
       "                                              text_stemmed  Category  \n",
       "0        peopl who post add me on snapchat must be dehy...         4  \n",
       "1        brianklaa as we see trump is danger to freepre...         8  \n",
       "2                              now issa is stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚         1  \n",
       "3        riskshow thekevinallison thx for the best time...         7  \n",
       "4                         still wait on those suppli liscu         4  \n",
       "...                                                    ...       ...  \n",
       "1451386  im so happi nowond the name of thi show happi ...         7  \n",
       "1451387  in everi circumt id like to be thank to the al...         7  \n",
       "1451388  there current two girl walk around the librari...         7  \n",
       "1451389  ah corpor life where you can date use just the...         7  \n",
       "1451390                         bless to be live sundayvib         7  \n",
       "\n",
       "[1451391 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20743eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the dataframe into a train and a test sections\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(twitter.text,twitter.emotion,\n",
    "                                                test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f548482d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=20000,\n",
       "                tokenizer=<function word_tokenize at 0x000002601FE84280>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW = CountVectorizer(tokenizer=nltk.word_tokenize, max_features=20000)\n",
    "\n",
    "BOW.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51600c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = BOW.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f200bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ðŸ˜•\" in feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fc0f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680793     thank you for the follow growhackwin we post i...\n",
       "815757     newsweek of cours realdonaldtrump doe becaus he a\n",
       "1124461    thehighfronti spacelectur cosmiccarol therogue...\n",
       "352702     hey maga peopl kellyannepol is your whitehous ...\n",
       "352523     a year later even my worst day look sunni in c...\n",
       "                                 ...                        \n",
       "259178                 ibizaradio fincaam today is simpli it\n",
       "1414414    thank to my follow you make me chaturb doomak ...\n",
       "131932     your mind will alway believ what you tell it f...\n",
       "671155      it is a day here happi humpday for those in path\n",
       "121958     djspinal albumsur feel like make cover on ever...\n",
       "Name: text_stemmed, Length: 1161112, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b113931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (1161112, 20000)\n",
      "y_train.shape:  (1161112,)\n",
      "x_test.shape:  (290279, 20000)\n",
      "y_test.shape:  (290279,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "x_train = BOW.transform(x_train)\n",
    "#y_train = y_train\n",
    "\n",
    "x_test = BOW.transform(x_test)\n",
    "#y_test = y_test\n",
    "\n",
    "print('x_train.shape: ', x_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('x_test.shape: ', x_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65355a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63955           disgust\n",
       "204069     anticipation\n",
       "1256026             joy\n",
       "561398            trust\n",
       "1094198           trust\n",
       "               ...     \n",
       "78967           disgust\n",
       "130772              joy\n",
       "760291              joy\n",
       "482430             fear\n",
       "1265785             joy\n",
       "Name: emotion, Length: 290279, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729a7b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adfd683d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680793            trust\n",
       "815757          sadness\n",
       "1124461        surprise\n",
       "352702          sadness\n",
       "352523            trust\n",
       "               ...     \n",
       "259178              joy\n",
       "1414414             joy\n",
       "131932              joy\n",
       "671155     anticipation\n",
       "121958     anticipation\n",
       "Name: emotion, Length: 1161112, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b365c7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 680793        trust\n",
      "815757      sadness\n",
      "1124461    surprise\n",
      "352702      sadness\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1161112,)\n",
      "y_test.shape:  (290279,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1161112, 8)\n",
      "y_test.shape:  (290279, 8)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    #return keras.utils.to_categorical(enc)\n",
    "    return keras.utils.np_utils.to_categorical(enc)   #Allison/Moo said so, because of version\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d476f2bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  20000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = x_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad041029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                640032    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642,408\n",
      "Trainable params: 642,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=32)(X)  # Original: 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=32)(H1)  # Original: 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3rd hidden layer   (CREATED BY ME)\n",
    "H1_W4 = Dense(units=32)(H2)  # Original: 64\n",
    "H4 = ReLU()(H1_W4)\n",
    "\n",
    "# output layer\n",
    "#Original output layer\n",
    "# H2_W3 = Dense(units=output_shape)(H2)  # 8\n",
    "# H3 = Softmax()(H2_W3)\n",
    "\n",
    "H2_W3 = Dense(units=output_shape)(H4)  # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1182a9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexc\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18143/18143 [==============================] - 120s 6ms/step - loss: 1.3125 - accuracy: 0.5252 - val_loss: 1.2690 - val_accuracy: 0.5413\n",
      "Epoch 2/5\n",
      "18143/18143 [==============================] - 127s 7ms/step - loss: 1.2174 - accuracy: 0.5594 - val_loss: 1.2554 - val_accuracy: 0.5468\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "\n",
    "#csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5 #25\n",
    "batch_size = 64 #32, 100\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max')\n",
    "callbacks = [es] #Early Stopping\n",
    "\n",
    "# training!\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = (x_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f70c4e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16290726, 0.05433075, 0.12348856, 0.02934447, 0.36641642,\n",
       "        0.10814618, 0.02665285, 0.1287135 ],\n",
       "       [0.00856169, 0.42061552, 0.01902953, 0.02902536, 0.17661281,\n",
       "        0.03539249, 0.01399135, 0.29677123],\n",
       "       [0.04705023, 0.02894008, 0.1784132 , 0.01496665, 0.2680302 ,\n",
       "        0.3118481 , 0.02767154, 0.12308005],\n",
       "       [0.00368886, 0.09511798, 0.07987957, 0.01113063, 0.51130396,\n",
       "        0.02117132, 0.05390204, 0.22380555],\n",
       "       [0.02132566, 0.03262321, 0.08668748, 0.02658406, 0.24574533,\n",
       "        0.46692622, 0.04343104, 0.07667705]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "pred_result = model.predict(x_test, batch_size=128) #128\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c1fb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'anticipation', 'sadness', 'joy', 'sadness'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4336e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.5468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 4)))\n",
    "\n",
    "#Best until now 0.5526"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfe230",
   "metadata": {},
   "source": [
    "### Predicting on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17cdfb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_test_data = pd.read_csv(\"twitter_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "150d0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test.shape:  (411972, 20000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.21698189e-02, 1.89265385e-01, 5.60832731e-02, 4.46962006e-02,\n",
       "        3.54148746e-01, 1.24623954e-01, 3.14107276e-02, 1.67601958e-01],\n",
       "       [2.69730808e-04, 6.75091565e-01, 3.00282700e-04, 2.80214613e-03,\n",
       "        3.44145484e-02, 3.85823892e-03, 7.97516026e-04, 2.82465994e-01],\n",
       "       [1.40540833e-02, 1.89538389e-01, 2.57310495e-02, 1.12396348e-02,\n",
       "        5.15705884e-01, 8.65562409e-02, 1.28159281e-02, 1.44358873e-01],\n",
       "       [9.16782767e-04, 5.98341644e-01, 6.13729528e-04, 2.90025328e-03,\n",
       "        2.58622348e-01, 2.43241200e-03, 1.17203291e-03, 1.35000780e-01],\n",
       "       [1.44831818e-02, 2.77871579e-01, 1.64835658e-02, 2.61475258e-02,\n",
       "        1.34920359e-01, 4.22948301e-02, 1.20408507e-02, 4.75758106e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_twitter = BOW.transform(twitter_test_data['text'])\n",
    "\n",
    "pred_result_test_data = model.predict(x_test_twitter, batch_size=128)\n",
    "\n",
    "print('x_test.shape: ', x_test_twitter.shape)\n",
    "pred_result_test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "925f2a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'anticipation', 'joy', 'anticipation', 'trust'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result_test_data = label_decode(label_encoder, pred_result_test_data)\n",
    "pred_result_test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d26ea1f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       emotion\n",
       "0       0x28b412           joy\n",
       "1       0x2de201  anticipation\n",
       "2       0x218443           joy\n",
       "3       0x2939d5  anticipation\n",
       "4       0x26289a         trust\n",
       "...          ...           ...\n",
       "411967  0x2913b4           joy\n",
       "411968  0x2a980e       sadness\n",
       "411969  0x316b80       sadness\n",
       "411970  0x29d0cb         anger\n",
       "411971  0x2a6a4f       sadness\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_df = pd.DataFrame(columns = [[\"id\",\"emotion\"]])\n",
    "upload_df[\"id\"] = twitter_test_data[\"tweet_id\"]\n",
    "upload_df[\"emotion\"] = pred_result_test_data\n",
    "upload_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b35df11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_df['emotion']\n",
    "upload_df.emotion.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd6be332",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_df.to_csv(\"./uploads/Keras_BOW_with_correct_stemmer.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fb979",
   "metadata": {},
   "source": [
    "## With Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0243a8",
   "metadata": {},
   "source": [
    "Use this one with only the BOW and the x_train, y_train BOW transformed. (using the category column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48894fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc5a91",
   "metadata": {},
   "source": [
    "## Another Keras Model Try (failed atm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4fc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim = x_train.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c2d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',#'binary_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47485a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20cc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
