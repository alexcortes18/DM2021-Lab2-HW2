{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42826d74",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856c3d4",
   "metadata": {},
   "source": [
    "#### Upload the CSV created from Kaggle Competition Preprocessing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012f51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#twitter = pd.read_csv(\"preprocessed_data.csv\")    #normal one used before\n",
    "#twitter = pd.read_csv(\"sorted_preprocessed_data.csv\")  #with lowest emotion scores\n",
    "#twitter = pd.read_csv(\"greater_than_some_score.csv\")\n",
    "twitter = pd.read_csv(\"final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0e0e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>883</td>\n",
       "      <td>0x292d69</td>\n",
       "      <td>forever daddys little girl  when daddy goes lo...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>forev daddi littl girl when daddi goe look for...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>391</td>\n",
       "      <td>0x271dff</td>\n",
       "      <td>the day before a big event and my team has eve...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>the day befor a big event and my team ha every...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507</td>\n",
       "      <td>0x305ac3</td>\n",
       "      <td>nikkinicolex i was going to do that this year ðŸ˜‚ðŸ˜©</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>nikkinicolex i wa go to do that thi year ðŸ˜‚ðŸ˜©</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>560</td>\n",
       "      <td>0x2415c8</td>\n",
       "      <td>donaldtrump    of obstruction of justice and i...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>donaldtrump of obstruct of justic and interfer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>348</td>\n",
       "      <td>0x378a4c</td>\n",
       "      <td>is holy and just therefore he must hate and p...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>is holi and just therefor he must hate and pun...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score  tweet_id                                               text  \\\n",
       "0     883  0x292d69  forever daddys little girl  when daddy goes lo...   \n",
       "1     391  0x271dff  the day before a big event and my team has eve...   \n",
       "2     507  0x305ac3  nikkinicolex i was going to do that this year ðŸ˜‚ðŸ˜©    \n",
       "3     560  0x2415c8  donaldtrump    of obstruction of justice and i...   \n",
       "4     348  0x378a4c   is holy and just therefore he must hate and p...   \n",
       "\n",
       "  identification       emotion  \\\n",
       "0          train       sadness   \n",
       "1          train         trust   \n",
       "2          train       sadness   \n",
       "3          train       sadness   \n",
       "4          train  anticipation   \n",
       "\n",
       "                                        text_stemmed  Category  \n",
       "0  forev daddi littl girl when daddi goe look for...         8  \n",
       "1  the day befor a big event and my team ha every...         2  \n",
       "2        nikkinicolex i wa go to do that thi year ðŸ˜‚ðŸ˜©         8  \n",
       "3  donaldtrump of obstruct of justic and interfer...         8  \n",
       "4  is holi and just therefor he must hate and pun...         4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_rows', 1000)\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a3a68",
   "metadata": {},
   "source": [
    "#### Dividing the dataframe into a train and a test sections. For the input section I tried with both \"text\" and \"text_stemmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20743eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(twitter.text,twitter.emotion,\n",
    "                                                test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21e679",
   "metadata": {},
   "source": [
    "#### Make a BOW for the top 20k - 25k (best results were in this range) max features. Using nltk.word_tokenize to accept emojis in the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f548482d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=25000,\n",
       "                tokenizer=<function word_tokenize at 0x000001AA8618FEE0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW = CountVectorizer(tokenizer=nltk.word_tokenize, max_features=25000)\n",
    "\n",
    "BOW.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2194442",
   "metadata": {},
   "source": [
    "#### Transform the \"text\" for both train and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b113931",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = BOW.transform(x_train)\n",
    "#y_train = y_train\n",
    "\n",
    "x_test = BOW.transform(x_test)\n",
    "#y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d80ba",
   "metadata": {},
   "source": [
    "#### Use 1 hot encoding to deal with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b365c7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d476f2bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  25000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = x_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a77ce5",
   "metadata": {},
   "source": [
    "#### Now I proceed to build the model. The lab model with hyperparameters modifications gave the best result. Even so, I also tried several other models you can see in the following pictures, but unfortunately this one gave me the best result. I also added another hidden layer, too.\n",
    "\n",
    "Other tries:\n",
    "![Snapshot](Models_tried.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad041029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 25000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                800032    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 802,408\n",
      "Trainable params: 802,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=32)(X)  # Original: 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=32)(H1)  # Original: 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3rd hidden layer   (CREATED BY ME)\n",
    "H1_W4 = Dense(units=32)(H2)  # Original: 64\n",
    "H4 = ReLU()(H1_W4)\n",
    "\n",
    "# output layer\n",
    "#Original output layer\n",
    "# H2_W3 = Dense(units=output_shape)(H2)  # 8\n",
    "# H3 = Softmax()(H2_W3)\n",
    "\n",
    "H2_W3 = Dense(units=output_shape)(H4)  # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a383e98",
   "metadata": {},
   "source": [
    "#### I tried also some different hyperparameters here, and added Early Stopping, which allow us 2 things. One is to stop overfitting by stopping the unnecesary epochs (when validation loss starts to increase), and also decreases training time by reducing those unnecesary epochs. In this model only 2 epochs run before it overfits, so then the model stops training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1182a9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexc\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18143/18143 [==============================] - 249s 13ms/step - loss: 1.3034 - accuracy: 0.5299 - val_loss: 1.2619 - val_accuracy: 0.5446\n",
      "Epoch 2/5\n",
      "18143/18143 [==============================] - 259s 14ms/step - loss: 1.1985 - accuracy: 0.5677 - val_loss: 1.2511 - val_accuracy: 0.5495\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "\n",
    "epochs = 5 #25\n",
    "batch_size = 64 #32, 100\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max')\n",
    "callbacks = [es] #Early Stopping\n",
    "\n",
    "# training!\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = (x_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc650cc",
   "metadata": {},
   "source": [
    "#### Predict on 'testing' data to see its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70c4e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.2603702e-04, 9.2258877e-01, 3.3979362e-03, 3.9225798e-03,\n",
       "        4.4723894e-02, 5.8233822e-03, 2.6032305e-03, 1.6214149e-02],\n",
       "       [5.0909184e-03, 7.9433107e-01, 3.7745996e-03, 2.9500829e-02,\n",
       "        1.1765296e-01, 1.4022427e-02, 1.1131980e-02, 2.4495170e-02],\n",
       "       [2.2109637e-02, 1.2432319e-01, 3.0477325e-02, 3.3597231e-02,\n",
       "        6.0769755e-01, 8.4985696e-02, 1.2158559e-02, 8.4650829e-02],\n",
       "       [1.1541225e-02, 1.9554177e-01, 3.7337534e-02, 4.9067251e-02,\n",
       "        3.1124482e-01, 9.6911915e-02, 2.3946233e-02, 2.7440926e-01],\n",
       "       [1.7788600e-04, 2.3848298e-01, 3.1241053e-04, 3.7969176e-03,\n",
       "        2.6607934e-01, 9.4746356e-04, 1.1908304e-03, 4.8901212e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "pred_result = model.predict(x_test, batch_size=128) #128\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a0048",
   "metadata": {},
   "source": [
    "#### Decode it to see the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c1fb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'anticipation', 'joy', 'joy', 'trust'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eecb428",
   "metadata": {},
   "source": [
    "#### Final training set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4336e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.5495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 4)))\n",
    "\n",
    "#Best until now 0.5526"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfe230",
   "metadata": {},
   "source": [
    "## Predicting on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b4475",
   "metadata": {},
   "source": [
    "#### Now i just upload the twitter_test_data.csv we got from the Kaggle Competition Preprocessing jupyter notebook and apply the recently acquired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17cdfb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_test_data = pd.read_csv(\"twitter_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e97a11",
   "metadata": {},
   "source": [
    "#### Transforming the testing data into a BOW and then predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "150d0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test.shape:  (411972, 25000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.2134670e-04, 5.8111531e-01, 3.5499751e-03, 2.2595141e-03,\n",
       "        1.9456419e-01, 5.0527016e-03, 2.0227081e-03, 2.1071419e-01],\n",
       "       [6.0479221e-04, 9.1848934e-01, 4.7078723e-04, 1.4382222e-03,\n",
       "        1.4037020e-02, 3.7493396e-03, 5.4701329e-03, 5.5740338e-02],\n",
       "       [2.3755964e-03, 5.7129842e-01, 1.2180364e-02, 5.7038465e-03,\n",
       "        3.4989572e-01, 1.4165925e-02, 5.2738823e-03, 3.9106205e-02],\n",
       "       [5.4947520e-04, 5.9566838e-01, 1.7403333e-03, 1.7238709e-03,\n",
       "        2.7456447e-01, 2.3667265e-03, 2.1701441e-03, 1.2121662e-01],\n",
       "       [1.7981682e-02, 4.0549445e-01, 3.6158517e-02, 3.4267507e-02,\n",
       "        1.4144489e-01, 7.1892999e-02, 1.1970902e-02, 2.8078905e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_twitter = BOW.transform(twitter_test_data['text'])\n",
    "\n",
    "pred_result_test_data = model.predict(x_test_twitter, batch_size=128)\n",
    "\n",
    "print('x_test.shape: ', x_test_twitter.shape)\n",
    "pred_result_test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9621b4d",
   "metadata": {},
   "source": [
    "#### Decoding the values to then be able to make a dataframe with meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925f2a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'anticipation', 'anticipation', 'anticipation',\n",
       "       'anticipation'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result_test_data = label_decode(label_encoder, pred_result_test_data)\n",
    "pred_result_test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c0cef",
   "metadata": {},
   "source": [
    "#### Making the DataFrame that will be uploaded to KAGGLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26ea1f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       emotion\n",
       "0       0x28b412  anticipation\n",
       "1       0x2de201  anticipation\n",
       "2       0x218443  anticipation\n",
       "3       0x2939d5  anticipation\n",
       "4       0x26289a  anticipation\n",
       "...          ...           ...\n",
       "411967  0x2913b4  anticipation\n",
       "411968  0x2a980e       sadness\n",
       "411969  0x316b80  anticipation\n",
       "411970  0x29d0cb         anger\n",
       "411971  0x2a6a4f       sadness\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_df = pd.DataFrame(columns = [[\"id\",\"emotion\"]])\n",
    "upload_df[\"id\"] = twitter_test_data[\"tweet_id\"]\n",
    "upload_df[\"emotion\"] = pred_result_test_data\n",
    "upload_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b35df11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_df['emotion']\n",
    "upload_df.emotion.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e33bc7",
   "metadata": {},
   "source": [
    "## Final Upload to KAGGLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd6be332",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_df.to_csv(\"./uploads/Keras_25k.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba165aa",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4edba",
   "metadata": {},
   "source": [
    "#### Extra try with logistic regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fb979",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0243a8",
   "metadata": {},
   "source": [
    "Use this one with only the BOW and the x_train, y_train BOW transformed. (But in the target here i needed the category column)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3e9c4",
   "metadata": {},
   "source": [
    "##### To use this one, change \"twitter.emotion\" to \"twitter.Category\" in the train-split data part, and avoid modeling the data with the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48894fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
